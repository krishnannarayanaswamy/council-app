{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnannarayanaswamy/astra-langchain-chatbot/blob/main/RAG_files_astra_langchain_dev_jam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g3jS6q_e_hv"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datastax/ragstack-ai/blob/main/examples/notebooks/langchain_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "posqsO9Ee_hz"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "You will need a vector-enabled Astra database and an OpenAI Account.\n",
        "\n",
        "* Create an [Astra vector database](https://docs.datastax.com/en/astra-serverless/docs/getting-started/create-db-choices.html).\n",
        "* Create an [OpenAI account](https://openai.com/)\n",
        "* Within your database, create an [Astra DB Access Token](https://docs.datastax.com/en/astra-serverless/docs/manage/org/manage-tokens.html) with Database Administrator permissions.\n",
        "* Get your Astra DB Endpoint:\n",
        "  * `https://<ASTRA_DB_ID>-<ASTRA_DB_REGION>.apps.astra.datastax.com`\n",
        "\n",
        "\n",
        "See the [Prerequisites](https://docs.datastax.com/en/ragstack/docs/prerequisites.html) page for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GfmW41Me_h0"
      },
      "source": [
        "## Setup\n",
        "`ragstack-ai` includes all the packages you need to build a RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "091VsLOge_h0",
        "nbmake": {
          "post_cell_execute": [
            "from conftest import before_notebook",
            "before_notebook()"
          ]
        },
        "outputId": "f4c361da-28f6-4660-c40b-576d635404eb"
      },
      "outputs": [],
      "source": [
        "! pip install -q ragstack-ai pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "fyaRUiW5e_h2",
        "nbmake": {
          "post_cell_execute": [
            "import string\n",
            "import random\n",
            "collection = ''.join(random.choice(string.ascii_lowercase) for _ in range(8))\n"
          ]
        },
        "outputId": "a7413c80-d929-4f9a-972c-cff25049289f",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Enter your settings for Astra DB and OpenAI:\n",
        "os.environ[\"ASTRA_DB_API_ENDPOINT\"] = input(\"Enter your Astra DB API Endpoint: \")\n",
        "os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"] = getpass(\"Enter your Astra DB Token: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "id": "cadgiv5je_h2",
        "tags": []
      },
      "source": [
        "## Create RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS0wrv-ue_h3"
      },
      "source": [
        "### Embedding Model and Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "JHgFsQyae_h3",
        "outputId": "2f31af4e-7e94-4ca1-995e-4ffc84245577",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_astradb import AstraDBVectorStore\n",
        "import os\n",
        "\n",
        "# Configure your embedding model and vector store\n",
        "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=1024)\n",
        "vstore = AstraDBVectorStore(\n",
        "    collection_name=\"thailand_dev_jam\",\n",
        "    embedding=embedding,\n",
        "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
        "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
        ")\n",
        "print(\"Astra vector store configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51KflVzIe_h5",
        "outputId": "d8892995-82da-41cf-a4c8-98ef4a7820e0"
      },
      "outputs": [],
      "source": [
        "# Retrieve the text of a short story that will be indexed in the vector store\n",
        "! curl https://raw.githubusercontent.com/CassioML/cassio-website/main/docs/frameworks/langchain/texts/amontillado.txt --output amontillado.txt\n",
        "SAMPLEDATA = [\"amontillado.txt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "yMfc7l-Te_h5",
        "outputId": "51c0320c-01f1-4a03-d10b-ebca71cb620c",
        "tags": [
          "skip-execution"
        ]
      },
      "outputs": [],
      "source": [
        "# Alternatively, provide your own file. However, you will want to update your queries to match the content of your file.\n",
        "\n",
        "# Upload sample file (Note: this cell assumes you are on Google Colab)\n",
        "# Local Jupyter notebooks can provide the path to their files directly by uncommenting and running just the next line).\n",
        "# SAMPLEDATA = [\"<path_to_file>\"]\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your own sample file:\")\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    SAMPLEDATA = uploaded\n",
        "else:\n",
        "    raise ValueError(\"Cannot proceed without Sample Data. Please re-run the cell.\")\n",
        "\n",
        "print(f\"Please make sure to change your queries to match the contents of your file!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcjYFfije_h5",
        "outputId": "5eb2b43f-6521-4d77-f60c-820593135b44"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Loop through each file and load it into our vector store\n",
        "documents = []\n",
        "#SAMPLEDATA = \"CCD_0046.pdf\"\n",
        "for filename in SAMPLEDATA:\n",
        "    path = os.path.join(os.getcwd(), filename)\n",
        "\n",
        "    # Supported file types are pdf and txt\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        pdf_loader = PyPDFLoader(path)\n",
        "        #Create document chunks & embeddings\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=64)\n",
        "        documents = pdf_loader.load_and_split(text_splitter=splitter)\n",
        "        print(documents)\n",
        "        print(f\"Documents from PDF: {len(documents)}.\")\n",
        "    elif filename.endswith(\".txt\"):\n",
        "        loader = TextLoader(path)\n",
        "        documents = loader.load_and_split()\n",
        "        print(f\"Processed txt file: {filename}\")\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {filename}\")\n",
        "\n",
        "# empty the list of file names in case this cell is run multiple times\n",
        "SAMPLEDATA = []\n",
        "\n",
        "print(f\"\\nProcessing done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01CqLd1re_h6",
        "outputId": "cd11d1e2-ed24-4dfa-a44e-3594719f2a39"
      },
      "outputs": [],
      "source": [
        "# Create embeddings by inserting your documents into the vector store.\n",
        "inserted_ids = vstore.add_documents(documents)\n",
        "print(f\"\\nInserted {len(inserted_ids)} documents.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "id": "aGb2bVfoe_h6",
        "outputId": "79a3819c-7d11-4821-8091-e52385abe139",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Checks your Collection to verify the Documents are embedded.\n",
        "print(vstore.astra_db.collection(\"thailand_dev_jam\").find())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R675lkne_h6"
      },
      "source": [
        "### Basic Retrieval\n",
        "\n",
        "Retrieve context from your vector database, and pass it to the model with a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "KSk0WHUHe_h6",
        "outputId": "021cddec-59e1-426d-d982-018813beeb32"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retriever = vstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer. Respond to the question in the same language as the user query.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Your answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "model = ChatOpenAI(model_name=\"gpt-4-0125-preview\")\n",
        "\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\n",
        "    \"Tell me more\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
